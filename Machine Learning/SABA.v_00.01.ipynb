{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Saba_v0.01.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9N_EzHEPl_BF",
        "bX39pKl7Eomm",
        "tH3iPGf5XlTv",
        "cyIaDh8TYvrP",
        "o3VeIQEaYCSc",
        "gDuju45icWUj"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python370jvsc74a57bd0c1f0e4dcb8a7a01e0bb3721a0191ecd19e112704c77ded682a13982bc8b9a525",
      "display_name": "Python 3.7.0 64-bit ('tf2.4': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9N_EzHEPl_BF"
      },
      "source": [
        "ReadMe\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Sahabat Asisten Belajar (SABA) or study companion is our capstone project for Bangkit 2021 For more information please visit or git hub [SABA](https://github.com/Bagoes-Heikhal/SABA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngd1HvWI8U98"
      },
      "source": [
        "# Chatbot with Deep NLP\n",
        "#Importing the libraries\n",
        "import numpy as np\n",
        "import tensorflow\n",
        "import re\n",
        "import time \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Bidirectional\n",
        "from google.colab import drive"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5SKxNklzyPc",
        "outputId": "eac737e1-fbd3-4b3e-a79c-3aca6fbded2c"
      },
      "source": [
        "print(tensorflow.__version__)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SV0Xk8FYVk55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "babbbc00-2609-4e94-8f8d-fd1c2a487041"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX39pKl7Eomm"
      },
      "source": [
        "**Data Preprocess**\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQEEgI_49Dd4"
      },
      "source": [
        "############## Import Dataset #############\n",
        "# questions = open ('./QuestionFisika.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        "# answers = open ('./AnswerFisika.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        "\n",
        "############## Untuk Drive ################## \n",
        "questions = open ('/content/drive/MyDrive/Colab Notebooks/SABA/data/QuestionFisika.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        "answers = open ('/content/drive/MyDrive/Colab Notebooks/SABA/data/AnswerFisika.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE9Pzxnlf3xU"
      },
      "source": [
        "####### Cleaning text dari tanda baca dan agar seragam #######\n",
        "def clean_text(text):\n",
        "    text = text.lower().strip()\n",
        "    text = re.sub(r'\" - \"+', \"\", text)\n",
        "    text = re.sub(r'\" +\"', \" \", text)\n",
        "    text = re.sub(r'\"^ \"', \"\", text)\n",
        "    text = re.sub(r'[-()\\\"#/@;:<>{}`+=~|.!?,]', \"\", text)\n",
        "    text = re.sub(r'[\\-\\-]', \"\", text)\n",
        "    text = re.sub(r'\\.\\.\\.', \"\", text)\n",
        "    text = re.sub(r'^- ', \"\", text)\n",
        "    return text"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1uxmRRBf6Jh"
      },
      "source": [
        "############### Petanyaan dan Jawaban clean ###############\n",
        "clean_questions = []\n",
        "for question in questions:\n",
        "    clean_questions.append(clean_text(question))\n",
        "    \n",
        "clean_answers = []\n",
        "for answer in answers:\n",
        "    clean_answers.append(clean_text(answer))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_ddCapBf8RP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8c40431-b143-4e51-f8cf-dddafab5703a"
      },
      "source": [
        "############### Untuk Tokenizer dan input embedding layers ############### \n",
        "word2count = {}\n",
        "for question in clean_questions:\n",
        "    for word in question.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "\n",
        "for answer in clean_answers:\n",
        "    for word in answer.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1\n",
        "\n",
        "print(len(word2count))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "840\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TagnKgBOf-h7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e936406-fa93-4969-8d55-adbe84c8de56"
      },
      "source": [
        "############ Membatasi batas minimum dictionary yang digunakan ##############\n",
        "threshold = 5\n",
        "\n",
        "vocab = {}\n",
        "word_num = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= threshold:\n",
        "        vocab[word] = word_num\n",
        "        word_num += 1\n",
        "\n",
        "print(len(vocab))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "840\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCOi_GSTgL3R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df737537-600c-433e-84e5-75af59287dd2"
      },
      "source": [
        "##### Memberikan Token Start of Sentence (SOS),\n",
        "##### End of Sentence (EOS)\n",
        "##### PAD untuk padding dan OUT untuk kata yang tidak ada di vocab\n",
        "\n",
        "for i in range(len(clean_answers)):\n",
        "    clean_answers[i] = '<SOS> ' + clean_answers[i] + ' <EOS>'\n",
        "\n",
        "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
        "x = len(vocab)\n",
        "for token in tokens:\n",
        "    vocab[token] = x\n",
        "    x += 1\n",
        "\n",
        "    \n",
        "print(clean_questions[0])\n",
        "print(clean_answers[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jangka sorong\n",
            "<SOS> jangka sorong adalah alat ukur yang mampu mengukur jarak kedalaman maupun ‘diameter dalam’ suatu objek dengan tingkat akurasi dan presisi yang sangat baik ±005 mm <EOS>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvI6xaKSgP_Y"
      },
      "source": [
        "###### inverse untuk decode ######\n",
        "inv_vocab = {w:v for v, w in vocab.items()}"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ohJ3sSXgtdz"
      },
      "source": [
        "###### Encoder Input ######\n",
        "encoder_inp = []\n",
        "for line in clean_questions:\n",
        "    lst = []\n",
        "    for word in line.split():\n",
        "        if word not in vocab:\n",
        "            lst.append(vocab['<OUT>'])\n",
        "        else:\n",
        "            lst.append(vocab[word])\n",
        "        \n",
        "    encoder_inp.append(lst)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f-dolU2gwvr"
      },
      "source": [
        "###### Decoder Input ######\n",
        "decoder_inp = []\n",
        "for line in clean_answers:\n",
        "    lst = []\n",
        "    for word in line.split():\n",
        "        if word not in vocab:\n",
        "            lst.append(vocab['<OUT>'])\n",
        "        else:\n",
        "            lst.append(vocab[word])        \n",
        "    decoder_inp.append(lst)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tH3iPGf5XlTv"
      },
      "source": [
        "**Encode Model**\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4jfinWFEp3_"
      },
      "source": [
        "############ Hyperparameters ##################\n",
        "max_input_len = 15\n",
        "lstm_layers = 400\n",
        "VOCAB_SIZE = len(vocab)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cZFP51jg8XM"
      },
      "source": [
        "#### Padding agar panjang encoder dan decoder sama ketika di training ####\n",
        "encoder_inp = pad_sequences(encoder_inp, maxlen = max_input_len, padding='post', truncating='post')\n",
        "decoder_inp = pad_sequences(decoder_inp, maxlen = max_input_len, padding='post', truncating='post')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRv63NTnhCiv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f28502f-b115-4057-e8df-44c6efa9a67f"
      },
      "source": [
        "#### Untuk output target learning ####\n",
        "\n",
        "decoder_final_output = []\n",
        "for i in decoder_inp:\n",
        "    decoder_final_output.append(i[1:])\n",
        "\n",
        "decoder_final_output = pad_sequences(decoder_final_output, maxlen = max_input_len, padding='post', truncating='post')\n",
        "decoder_final_output = to_categorical(decoder_final_output, len(vocab))\n",
        "print(decoder_final_output.shape)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3728, 15, 844)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htyXVDtxhQNb"
      },
      "source": [
        "##### Embedding layer utama ####\n",
        "embedding = Embedding(VOCAB_SIZE + 1,\n",
        "                      output_dim = 50, \n",
        "                      input_length = max_input_len,\n",
        "                      trainable = True,\n",
        "                      mask_zero = True \n",
        "                      )"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eexRcw3ihRfe"
      },
      "source": [
        "########### Encode  #############\n",
        "\n",
        "########### Encoder #############\n",
        "e_input = Input(shape=(max_input_len, ))\n",
        "e_embeded = embedding(e_input)\n",
        "e_lstm = LSTM(lstm_layers, return_sequences=True, return_state=True)\n",
        "e_op, h_state, c_state = e_lstm(e_embeded)\n",
        "e_states = [h_state, c_state]\n",
        "\n",
        "########### Decoder #############\n",
        "d_input = Input(shape=(max_input_len, ))\n",
        "d_embeded = embedding(d_input)\n",
        "d_lstm = LSTM(lstm_layers, return_sequences=True, return_state=True)\n",
        "d_output, _, _ = d_lstm(d_embeded, initial_state = e_states)\n",
        "d_dense = Dense(VOCAB_SIZE, activation='softmax')\n",
        "d_outputs = d_dense(d_output)\n",
        "\n",
        "model = Model([e_input, d_input], d_outputs)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyIaDh8TYvrP"
      },
      "source": [
        "**TRAINING**\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWwa_JkvhXfX"
      },
      "source": [
        "model.compile(optimizer='adam', \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['acc'])\n",
        "\n",
        "# from keras.utils.vis_utils import plot_model\n",
        "# plot_model(model, show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXCY3WqiGvkz"
      },
      "source": [
        "############################ Untuk Memulai Training ############################\n",
        "\n",
        "# model.fit([encoder_inp, decoder_inp], decoder_final_output, epochs=50, validation_split=0.2)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3VeIQEaYCSc"
      },
      "source": [
        "**Inference**\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBSQLiRjh49E"
      },
      "source": [
        "######### Decode  #############\n",
        "\n",
        "######### Encoder #############\n",
        "enc_model = Model([e_input], e_states)\n",
        "\n",
        "######### Decoder #############\n",
        "d_input_h = Input(shape=(lstm_layers,))\n",
        "d_input_c = Input(shape=(lstm_layers,))\n",
        "d_states_inputs = [d_input_h, d_input_c]\n",
        "d_outputs, d_state_h, d_state_c = d_lstm(d_embeded, initial_state = d_states_inputs)\n",
        "d_states = [d_state_h, d_state_c]\n",
        "dec_model = Model([d_input] + d_states_inputs, [d_outputs] + d_states)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU0-leDoX9dT"
      },
      "source": [
        "############# Untuk Meload Model & weigth ##################\n",
        "# enc_model.load_weights('./enc_model_fisika.h5')\n",
        "# dec_model.load_weights('./dec_model_fisika.h5')\n",
        "# model.load_weights('./main_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82VRbGMadbR0"
      },
      "source": [
        "############ plot Model ############\n",
        "\n",
        "# from keras.utils.vis_utils import plot_model\n",
        "# plot_model(enc_model, show_shapes=True, show_layer_names=True)\n",
        "# plot_model(dec_model, show_shapes=True, show_layer_names=True)\n",
        "# enc_model.summary()\n",
        "# dec_model.summary()"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5kIy3UuX9dU"
      },
      "source": [
        "########## Untuk mengholah data input sebelum di inference ##########\n",
        "\n",
        "def input_sentence(text):\n",
        "    user_ = clean_text(text)\n",
        "    user = [user_]\n",
        "\n",
        "    inp_sentence = []\n",
        "    for sentence in user:\n",
        "        lst = []\n",
        "        for y in sentence.split():\n",
        "            try:\n",
        "                lst.append(vocab[y])\n",
        "            except:\n",
        "                lst.append(vocab['<OUT>'])\n",
        "        inp_sentence.append(lst)\n",
        "    \n",
        "    inputs_sentence = pad_sequences(inp_sentence, max_input_len, padding='post')\n",
        "    states_value = enc_model.predict(inputs_sentence)\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = vocab['<SOS>']\n",
        "\n",
        "    return target_seq, states_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVS8TUXSX9dU"
      },
      "source": [
        "############# BOT SABA #################\n",
        "user_ = \"\"\n",
        "while user_ != 'quit':\n",
        "    user_  = input(\"you : \")\n",
        "    target_seq, states_value = input_sentence(user_)\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded = ''\n",
        "\n",
        "    while not stop_condition :\n",
        "        output_tokens , h, c= dec_model.predict([target_seq] + states_value )\n",
        "        input_tokens = d_dense(output_tokens)\n",
        "        word_index = np.argmax(input_tokens[0, -1, :])\n",
        "\n",
        "        ### Invers dari angka ke word index dalam vocab ###\n",
        "        word = inv_vocab[word_index] + ' '\n",
        "        if word != '<EOS> ':\n",
        "            decoded += word  \n",
        "        if word == '<EOS> ' or len(decoded.split()) > max_input_len:\n",
        "            stop_condition = True \n",
        "\n",
        "        target_seq = np.zeros((1 , 1))  \n",
        "        target_seq[0 , 0] = word_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    print(\"user :\", user_)\n",
        "    print(\"chatbot attention : \", decoded)\n",
        "    print(\"==============================================\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDuju45icWUj"
      },
      "source": [
        "**Saving Model & Weights**\n",
        "---\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Tex_RwSiJcB"
      },
      "source": [
        "# enc_model.save_weights('./enc_model_fisika.h5')\n",
        "# dec_model.save_weights('./dec_model_fisika.h5')\n",
        "# model.save_weights('./main_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}